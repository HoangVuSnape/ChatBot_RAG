{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramesters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data_path = \"data\"\n",
    "vector_db_path = \"vectorstores/db_faiss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_from_text(raw_text):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=512, \n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "        )\n",
    "    chunks = text_splitter.split_text(raw_text)\n",
    "    \n",
    "    #Embeddings\n",
    "    embedding_model = GPT4AllEmbeddings(model_file=\"../models/all-MiniLM-L6-v2-f16.gguf\")\n",
    "    \n",
    "    #Put into Faiss Vector db_faiss\n",
    "    db = FAISS.from_texts(texts=chunks, embedding=embedding_model)\n",
    "    db.save_local(vector_db_path)\n",
    "    print(\"Success\")\n",
    "    return db\n",
    "\n",
    "def create_db_from_files(folder_path='../data'):\n",
    "    #Load all data in data folder\n",
    "    loader = DirectoryLoader(folder_path, glob=\"*.pdf\", loader_cls = PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    embedding_model = GPT4AllEmbeddings(model_file=\"../models/all-MiniLM-L6-v2-f16.gguf\")\n",
    "    db = FAISS.from_documents(chunks, embedding=embedding_model)\n",
    "    db.save_local(vector_db_path)\n",
    "    print(\"Success\")\n",
    "    return db\n",
    "    \n",
    "def create_db_from__one_file(pdf_file):\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50))\n",
    "    embedding_model = GPT4AllEmbeddings(model_file=\"../models/all-MiniLM-L6-v2-f16.gguf\")\n",
    "    db = FAISS.from_documents(pages, embedding=embedding_model)\n",
    "    db.save_local(vector_db_path)\n",
    "    print(\"Success\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_vectorstores(folder_path = \"./vectorstores\"):\n",
    "    \"\"\"\n",
    "    Remove all files and subdirectories within the specified folder.\n",
    "\n",
    "    Args:\n",
    "    - folder_path (str): Path to the folder whose contents will be removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Iterate over all files and subdirectories within the folder\n",
    "        for item in os.listdir(folder_path):\n",
    "            item_path = os.path.join(folder_path, item)\n",
    "            # If the item is a file, remove it\n",
    "            if os.path.isfile(item_path):\n",
    "                os.remove(item_path)\n",
    "            # If the item is a directory, remove it recursively\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)\n",
    "        print(f\"All contents within {folder_path} have been removed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while removing contents: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_db_from_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "Success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x17dff41a610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_db_from_files(\"../data\")\n",
    "\n",
    "    \n",
    "## transfrom text into vectorDB\n",
    "raw_text = \"\"\" Transformer attention thông thường thực hiện attention trên toàn bộ \n",
    "feature map nó dẫn đến độ phức tạp của thuật toán tăng cao khi spatial size của feature map tăng. \n",
    "Tác giả đưa ra một kiểu attention mới mà chỉ attend \n",
    "vào một số sample locations (sample locations này cũng không cố định mà được học trong \n",
    "quá trình training tương tự như trong deformable convolution) \n",
    "qua đó giúp giảm độ phức tạp của thuật toán và làm giảm thời gian training mô hình. \"\"\"\n",
    "create_db_from_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All contents within ./vectorstores have been removed successfully.\n"
     ]
    }
   ],
   "source": [
    "remove_all_vectorstores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfrom 1 pdf file into vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x17d89a25f50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_file = \"../data/Ebook Copywriting - Minh Xin Chào.pdf\"\n",
    "create_db_from__one_file(pdf_file)\n",
    "# data\\Ebook Copywriting - Minh Xin Chào.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All contents within ./vectorstores have been removed successfully.\n"
     ]
    }
   ],
   "source": [
    "remove_all_vectorstores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfrom folder contain pdfs file into vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x17d89971750>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_db_from_files('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All contents within ./vectorstores have been removed successfully.\n"
     ]
    }
   ],
   "source": [
    "remove_all_vectorstores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
